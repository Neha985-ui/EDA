{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "201c50dc-5ed9-4116-a704-48bb95ddeeee",
   "metadata": {},
   "source": [
    "Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd51d4b2-195a-4513-9bf6-86a6f540fe7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is a parameter?\n",
    "'''\n",
    "In Machine Learning (ML), a parameter refers to the internal variables or coefficients in a model that are learned from\n",
    "the data during the training process. These parameters are crucial for the model's behavior and prediction ability. The goal \n",
    "of training an ML model is to adjust these parameters so that the model can make accurate predictions on new, unseen data.\n",
    "Types of Parameters in ML:\n",
    "Weights: In most machine learning models, especially in neural networks and linear regression, weights are parameters that \n",
    "are adjusted during training. They control the importance of the input features in predicting the output.\n",
    "\n",
    "Bias: A bias term is added to the model to adjust the output regardless of the input. It helps shift the activation\n",
    "function in models like neural networks. \n",
    "Hyperparameters are set before training begins and are not learned from the data. Instead, they control the training process\n",
    "or the model’s architecture. For instance\n",
    "Learning rate (how fast the model adjusts parameters)\n",
    "Number of layers in a neural network\n",
    "Number of trees in a random forest\n",
    "Regularization strength (which helps prevent overfitting)\n",
    "Unlike parameters, hyperparameters are typically chosen manually through experimentation, grid search, or optimization \n",
    "techniques like random search or Bayesian optimization.\n",
    "Example: Neural Network Parameters\n",
    "In a neural network, the parameters include the weights and biases of each neuron in every layer of the network. During \n",
    "the training process, the network updates these parameters to reduce the error (or loss) between its predictions and \n",
    "the actual target values.\n",
    "\n",
    "Key Points:\n",
    "Parameters are the model’s internal variables that are learned from the data.\n",
    "During training, the goal is to adjust these parameters using an optimization technique (such as gradient descent) \n",
    "to minimize the loss function (error between predictions and actual values).\n",
    "Hyperparameters, on the other hand, are not learned and control the model's configuration or training process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c7c5c1-42f3-4557-bc27-36a91cb33174",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is correlation?\n",
    "#What does negative correlation mean?\n",
    "'''\n",
    "\n",
    "Correlation is a statistical measure that describes the strength and direction of a relationship between two variables. \n",
    "It quantifies how changes in one variable are associated with changes in another. Correlation can be either positive,\n",
    "negative, or zero, depending on how the variables move in relation to each other.\n",
    "\n",
    "-Positive Correlation: As one variable increases, the other also increases, and vice versa.\n",
    "-Negative Correlation: As one variable increases, the other decreases, and vice versa.\n",
    "-Zero or No Correlation: Changes in one variable are not associated with changes in the other.\n",
    "\n",
    "The correlation coefficient is the value that quantifies the strength and direction of the correlation. It typically\n",
    "ranges from **-1 to 1**:\n",
    "- **+1**: Perfect positive correlation (the two variables move in the same direction in a perfectly linear fashion).\n",
    "- **-1**: Perfect negative correlation (the two variables move in opposite directions in a perfectly linear fashion).\n",
    "- **0**: No correlation (no linear relationship between the variables).\n",
    "\n",
    "Negative Correlation\n",
    "\n",
    "A negative correlation means that as one variable increases, the other tends to decrease. In other words, the two variables\n",
    "move in opposite directions. \n",
    "\n",
    "For example:\n",
    "- Height and weight in a population: As height increases, weight might decrease in certain age groups or conditions\n",
    "(e.g., very young children or certain health conditions). In most cases, though, height and weight have a positive \n",
    "correlation, but in some edge cases, a negative correlation could exist.\n",
    "- Outdoor temperature and heating costs: As the temperature increases (warmer weather), the cost of heating your home \n",
    "typically decreases, which is an example of a negative correlation.\n",
    "\n",
    "The strength of the negative correlation is determined by the **correlation coefficient:\n",
    "- **-1** represents a perfect negative correlation, meaning that for every increase in one variable, the other decreases in \n",
    "a perfectly predictable manner.\n",
    "- **-0.5** indicates a moderate negative correlation, where the variables tend to move in opposite directions, but not in a\n",
    "perfectly predictable way.\n",
    "- A value closer to **0** (such as **-0.1**) indicates a weak negative correlation, where changes in one variable have very \n",
    "little effect on the other.\n",
    "\n",
    "Example of Negative Correlation:\n",
    "\n",
    "Consider the relationship between **study time** and **exam anxiety**. \n",
    "- As the **study time increases**, exam anxiety might **decrease** because the student feels more prepared. This could \n",
    "indicate a negative correlation between the two variables.\n",
    "- A **correlation coefficient** closer to **-1** would suggest a strong inverse relationship, while a value closer to \n",
    "**0** would indicate a weak or no relationship.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93634568-1f81-4f2f-b67b-e3630d3ad726",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Machine Learning. What are the main components in Machine Learning?\n",
    "'''\n",
    "Machine Learning (ML) is a subfield of artificial intelligence (AI) focused on building algorithms that allow computers to \n",
    "learn from and make predictions or decisions based on data. Instead of being explicitly programmed for every task, machine \n",
    "learning models improve their performance over time by learning from patterns in data.In ML, algorithms use statistical \n",
    "techniques to identify patterns in data, then apply those patterns to make predictions or decisions without human intervention.\n",
    "This ability to automatically learn and improve from experience is what makes machine learning powerful.\n",
    "Main Components in Machine Learning\n",
    "Data:\n",
    "Data is the foundation of machine learning. It includes the examples or observations the model will learn from. \n",
    "The data can be in various forms such as text, images, numbers, or audio. Data is typically divided into training data\n",
    "(to train the model) and test data (to evaluate the model’s performance).\n",
    "Model:\n",
    "The model is the mathematical or computational structure that makes predictions or decisions based on data. It is defined \n",
    "by algorithms that process the data and learn patterns from it. Examples of models include linear regression, decision trees, \n",
    "neural networks, and support vector machines.\n",
    "Algorithms:\n",
    "Machine learning algorithms are the specific techniques or methods that the model uses to learn from data. Common types of \n",
    "algorithms include:\n",
    "Supervised learning algorithms (e.g., linear regression, decision trees)\n",
    "Unsupervised learning algorithms (e.g., k-means clustering, PCA)\n",
    "Reinforcement learning algorithms (e.g., Q-learning, deep Q-networks)\n",
    "Features:\n",
    "Features are the individual measurable properties or characteristics of the data. In supervised learning, features are the\n",
    "input variables used to make predictions. For example, in a house price prediction model, features might include the size of \n",
    "the house, number of rooms, or location.\n",
    "Labels:\n",
    "Labels are the target outcomes or values that the model tries to predict. In supervised learning, these are the \"correct\n",
    "answers\" provided with the training data. For example, in a classification task (e.g., spam detection), the label might be \"spam\" or \"not spam.\"\n",
    "Training:\n",
    "Training refers to the process of feeding data to the model and allowing it to adjust its internal parameters (weights) to \n",
    "minimize errors or optimize performance. During training, the algorithm learns from the data to identify patterns and make better predictions over time.\n",
    "Testing:\n",
    "After training, the model is evaluated using a separate set of data known as the test data. This data has not been seen by \n",
    "the model during training and is used to assess the generalization ability of the model—how well it performs on new, unseen data.\n",
    "Evaluation Metrics:\n",
    "These metrics are used to assess the performance of a machine learning model. Common metrics include:\n",
    "Accuracy (for classification tasks)\n",
    "Mean Squared Error (MSE) (for regression tasks)\n",
    "Precision, Recall, F1-score (for classification tasks)\n",
    "AUC-ROC (for classification tasks)\n",
    "Confusion Matrix (for classification tasks)\n",
    "Optimization:\n",
    "\n",
    "Optimization is the process of adjusting the parameters or weights of the model to minimize or maximize a certain objective,\n",
    "usually the loss function. Common optimization methods include gradient descent, stochastic gradient descent, and genetic\n",
    "algorithms.\n",
    "Hyperparameters:\n",
    "Hyperparameters are configuration settings that govern the training process or the structure of the model. Examples include\n",
    "learning rate, number of hidden layers in a neural network, or the number of clusters in a clustering model. Hyperparameters \n",
    "are set before training and are typically tuned to improve model performance.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6821c21e-c57e-4a15-afa7-dd1e95e9d984",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How does loss value help in determining whether the model is good or not?\n",
    "'''\n",
    "The loss value is a crucial metric in machine learning that helps evaluate how well a model is performing. It quantifies the \n",
    "difference between the predicted outputs of the model and the actual target values, showing how much error the model is making\n",
    "A low loss indicates that the model is making accurate predictions, while a high loss suggests that the model is not learning \n",
    "effectively. During the training process, the goal is to minimize the loss value by adjusting the model's parameters. If the \n",
    "loss continues to decrease over time, it means the model is improving and learning from the data. On the other hand, if the \n",
    "loss value stagnates or increases, it could indicate that the model is struggling to learn the patterns in the data or is\n",
    "overfitting or underfitting.\n",
    "The loss value is also helpful for detecting overfitting and underfitting. If the model has a high loss on both training and\n",
    "validation datasets, it suggests underfitting, meaning the model is too simple to capture the patterns in the data. In \n",
    "contrast, if the model shows a very low loss on the training set but a high loss on the test or validation data, it may \n",
    "indicate overfitting. In this case, the model has learned the training data too well, including noise or irrelevant details,\n",
    "but is unable to generalize to new, unseen data. Therefore, a good model should have a relatively low loss on both training\n",
    "and test datasets, showing that it can generalize well.\n",
    "Additionally, loss values can be used to compare different models or algorithms. By evaluating the loss on the test data, one\n",
    "can determine which model is performing better. The model with the lowest loss is typically preferred, but it's essential to\n",
    "ensure that it is not overfitting by checking the test loss. For different types of machine learning tasks, there are specific\n",
    "loss functions that help in the optimization process. For example, regression tasks commonly use mean squared error (MSE),\n",
    "while classification tasks often use cross-entropy loss. Choosing the appropriate loss function is crucial for the model’s \n",
    "learning process and its ability to make accurate predictions.\n",
    "Ultimately, loss functions guide the model’s learning process by providing feedback on its predictions, which is essential for\n",
    "improving the model’s performance over time. By minimizing the loss, the model becomes better at making predictions, and\n",
    "evaluating the loss allows us to determine if the model is generalizing well or if adjustments are needed. Thus, the loss\n",
    "value is an integral part of training, optimizing, and assessing a machine learning model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71d7865-789e-4a51-b833-8af6385570c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What are continuous and categorical variables?\n",
    "'''\n",
    "Continuous and categorical variables are two broad types of data used in statistical analysis and machine learning, each with\n",
    "distinct characteristics. Continuous variables are those that can take an infinite number of values within a given range. \n",
    "These variables are typically quantitative and represent measurements or counts that can be broken down into smaller\n",
    "increments. For example, height, weight, temperature, or time are continuous variables, as they can have any value within a\n",
    "range (such as 5.7 kg, 5.75 kg, or 5.753 kg). Continuous variables are often used in tasks that involve prediction, such as \n",
    "regression analysis, where the goal is to predict a specific value.\n",
    "On the other hand, categorical variables are those that represent discrete categories or groups. These variables can take a\n",
    "limited number of values, each corresponding to a different category or class. Categorical variables are qualitative in nature\n",
    "and are often used in classification tasks. For example, variables such as gender (male, female), color (red, blue, green),\n",
    "or marital status (single, married, divorced) are categorical because they divide the data into distinct groups or labels. \n",
    "Categorical variables can either be nominal, where the categories have no inherent order (like color), or ordinal, where the \n",
    "categories have a meaningful order or ranking (like education level: high school, bachelor's, master's, PhD).\n",
    "In summary, continuous variables are numerical and can take on an infinite number of values, whereas categorical variables\n",
    "are qualitative and consist of a finite number of distinct categories. These differences influence the types of analyses and \n",
    "algorithms used in statistical modeling and machine learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32911c0e-e01d-4252-aa5e-4d40df62097c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
    "'''\n",
    "Handling categorical variables in machine learning is crucial because most machine learning algorithms require numerical input.\n",
    "Categorical variables represent distinct groups or classes, and these need to be transformed into a numerical format before \n",
    "they can be used in models. There are several techniques to handle categorical variables, each suited for different types of \n",
    "data and tasks. One common method is **One-Hot Encoding**, where each category is represented by a binary vector. Each \n",
    "category of a variable is converted into a separate binary feature, with a 1 indicating the presence of that category and a 0 \n",
    "indicating its absence. For instance, if the variable is \"Color\" with values \"Red\", \"Blue\", and \"Green\", one-hot encoding \n",
    "would create three new columns, one for each color, with 1s and 0s indicating which color is present in a particular \n",
    "observation.\n",
    "Another technique is **Label Encoding**, which assigns a unique integer to each category. This method is more efficient in\n",
    "terms of space but can introduce an unintended ordinal relationship between the categories, which may not be appropriate for\n",
    "nominal variables. For example, \"Red\" might be encoded as 1, \"Blue\" as 2, and \"Green\" as 3, even though the color names have \n",
    "no inherent order.\n",
    "**Target Encoding**, or **Mean Encoding**, is a method where each category is replaced by the mean of the target variable for \n",
    "that category. This technique is particularly useful in supervised learning tasks, especially when there are a large number of\n",
    "categories, as it can help reduce the dimensionality of the data. However, target encoding can lead to overfitting if not \n",
    "properly regularized.\n",
    "Lastly, for high-cardinality categorical variables (variables with many categories), techniques like **Frequency Encoding** or\n",
    "**Binary Encoding** are used. Frequency encoding replaces each category with the frequency of its occurrence in the dataset,\n",
    "while binary encoding transforms the categories into binary digits, which is more compact than one-hot encoding.\n",
    "Choosing the right encoding method depends on the type of machine learning model being used, the nature of the categorical\n",
    "variable, and the problem at hand. Some models, like decision trees, can handle categorical variables directly without the\n",
    "need for encoding. However, for most machine learning models, such as linear regression or neural networks, converting\n",
    "categorical variables into numerical representations is necessary to make the data usable for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68267d0-68bc-4c95-9dd6-8c35db5ae71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What do you mean by training and testing a dataset?\n",
    "'''\n",
    "Training and testing a dataset are two key stages in the machine learning process. **Training a dataset** refers to the\n",
    "process of using a set of data to teach a machine learning model to recognize patterns, learn relationships, and make\n",
    "predictions. During training, the model is fed input data (features) along with the correct output (labels or target values).\n",
    "The model adjusts its internal parameters to minimize the error between its predictions and the actual outcomes. The primary \n",
    "goal of this phase is to enable the model to learn and generalize from the data to make accurate predictions on unseen data. \n",
    "This is where the model \"learns\" from examples, and the performance of the model is evaluated based on how well it can fit the\n",
    "training data.\n",
    "**Testing a dataset**, on the other hand, occurs after the model has been trained. It involves using a separate set of data, \n",
    "known as the **test data**, to evaluate the model's performance. The test data consists of input data that the model has not \n",
    "seen before, and it serves as a proxy for how the model will perform on new, unseen data in real-world scenarios. The key here\n",
    "is that the model does not learn from the test data; it only makes predictions based on what it has learned during training. \n",
    "By comparing the model's predictions with the actual values in the test dataset, we can assess its accuracy, generalization \n",
    "ability, and overall effectiveness. The performance on the test data provides insight into whether the model is overfitting,\n",
    "underfitting, or appropriately generalizing.\n",
    "In summary, training involves teaching the model with labeled data, and testing involves evaluating how well the model \n",
    "performs on unseen data to ensure it can generalize effectively. Both stages are critical for developing reliable and accurate\n",
    "machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616852b2-9a0b-48a3-8ccd-a16f4c0dcbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is sklearn.preprocessing?\n",
    "'''\n",
    "**sklearn.preprocessing** is a module in the **scikit-learn** library, which is widely used for preprocessing and transforming\n",
    "data before it is fed into machine learning models. The primary purpose of this module is to standardize or normalize the\n",
    "features of the dataset, making them suitable for a variety of machine learning algorithms. This is important because many \n",
    "machine learning models perform better or require input data to be on a similar scale, especially when features have different\n",
    "units or magnitudes. The **preprocessing** module includes various tools and techniques for tasks such as scaling numerical\n",
    "data, encoding categorical variables, and handling missing values.\n",
    "For instance, **StandardScaler** is a tool in sklearn.preprocessing that standardizes features by removing the mean and\n",
    "scaling to unit variance, ensuring all features contribute equally to the model. **MinMaxScaler** is another technique that \n",
    "scales the features to a specified range, typically between 0 and 1. Similarly, categorical data can be transformed using\n",
    "methods like **OneHotEncoder** or **LabelEncoder**, which convert categorical variables into a format that can be used by\n",
    "machine learning algorithms. The module also provides tools for handling missing data through imputation techniques,\n",
    "such as **SimpleImputer**, which fills in missing values with statistical methods like the mean, median, or mode.\n",
    "In essence, **sklearn.preprocessing** is a crucial part of preparing raw data for machine learning, helping to ensure that \n",
    "the data is clean, consistent, and properly formatted for effective learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c45df3-46bf-419e-bcbb-b6540680e144",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is a Test set?\n",
    "'''\n",
    "A **test set** is a subset of the dataset used to evaluate the performance of a machine learning model after it has been\n",
    "trained. It consists of data that the model has not seen during the training phase. The purpose of the test set is to assess\n",
    "how well the model generalizes to new, unseen data, which is crucial for determining whether the model is overfitting or\n",
    "underfitting.\n",
    "When building a machine learning model, the available data is typically split into at least two parts: the **training set** \n",
    "and the **test set**. The training set is used to teach the model, while the test set serves as a final evaluation to simulate\n",
    "how the model would perform in real-world applications, where it encounters new, unseen data. The test set helps gauge key\n",
    "performance metrics, such as accuracy, precision, recall, and other relevant metrics, depending on the problem.\n",
    "A key principle in using a test set is that the model should not have access to this data during training or hyperparameter\n",
    "tuning, as doing so could lead to biased performance metrics and undermine the test set’s role as a true measure of\n",
    "generalization. The test set helps ensure that the model isn't just memorizing the training data (a problem known as \n",
    "**overfitting**), but instead is learning general patterns that can be applied to new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e368915c-0a94-4f18-885d-925d32dfa722",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How do we split data for model fitting (training and testing) in Python?\n",
    "#How do you approach a Machine Learning problem?\n",
    "'''\n",
    "To split data for model fitting in Python, the most common approach is to use the `train_test_split` function from the \n",
    "**scikit-learn** library. This function helps divide the dataset into two parts: a training set and a test set. The training\n",
    "set is used to train the model, while the test set is reserved for evaluating the model's performance on unseen data.\n",
    "For example, you can use the following code to split your data: `X_train, X_test, y_train, y_test = train_test_split\n",
    "(X, y, test_size=0.2, random_state=42)`. Here, `X` represents the feature data, `y` represents the target labels, and the\n",
    "`test_size=0.2` means 20% of the data will be used for testing, with the remaining 80% used for training. The `random_state`\n",
    "ensures that the split is reproducible, so the results can be consistent across different runs.\n",
    "When approaching a machine learning problem, the first step is to define the problem clearly—whether it's a classification or\n",
    "regression task—and understand what you’re trying to predict. Next, you collect and prepare the data, which may involve \n",
    "cleaning, handling missing values, and transforming the data into a usable format. Once the data is ready, you perform \n",
    "**Exploratory Data Analysis (EDA)** to uncover trends, patterns, and relationships within the dataset. After understanding the\n",
    "data, you split it into training and test sets to ensure the model can be evaluated on data it hasn't seen before. Following \n",
    "this, you select a suitable machine learning model, such as a decision tree for classification or linear regression for \n",
    "regression tasks. You then train the model using the training data, after which you evaluate its performance on the test set\n",
    "using relevant metrics like accuracy for classification or mean squared error for regression. If needed, you can fine-tune the\n",
    "model using hyperparameter optimization methods like grid search. Once the model is validated, you select the best-performing\n",
    "model and deploy it into production for real-world use. Finally, it's important to monitor the model's performance regularly,\n",
    "as it may require updates or retraining over time to maintain accuracy with new data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627b3848-ed21-4103-b141-1c83b3ea41a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Why do we have to perform EDA before fitting a model to the data?\n",
    "'''\n",
    "Performing **Exploratory Data Analysis (EDA)** before fitting a model to the data is essential because it helps you gain a \n",
    "deeper understanding of the dataset, which in turn allows you to make more informed decisions during the modeling process.\n",
    "EDA provides insights into the underlying structure, patterns, and relationships within the data, which can reveal critical\n",
    "information such as the distribution of variables, potential outliers, missing values, and correlations between features. By\n",
    "exploring the data visually and statistically, you can identify issues like class imbalances, skewed distributions, or \n",
    "multicollinearity that might affect model performance. EDA also allows you to detect anomalies or errors in the data, such as\n",
    "duplicates or incorrect data types, which, if left unaddressed, could lead to inaccurate or biased model predictions. \n",
    "Furthermore, through EDA, you can identify the most relevant features for the model and decide whether to apply \n",
    "transformations, scaling, or encoding techniques to improve the model’s performance. In essence, EDA serves as a diagnostic\n",
    "tool that helps you prepare the data for modeling, ensuring that the model is trained on clean, relevant, and well-understood \n",
    "data, which increases the likelihood of achieving accurate and reliable predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f283fd02-331d-4504-9e99-f10b43d2f4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation coefficient: 1.0\n",
      "P-value: 0.0\n"
     ]
    }
   ],
   "source": [
    "#How can you find correlation between variables in Python?\n",
    "'''\n",
    "In Python, you can find the correlation between variables using the **pandas** library, which provides an easy-to-use\n",
    "function called `corr()`. This function computes the correlation coefficient between numeric columns in a DataFrame.\n",
    "To begin, you first need to load your data into a pandas DataFrame. Once the data is loaded, you can use `df.corr()` to\n",
    "calculate the pairwise correlation between all numeric variables in the dataset. This will return a correlation matrix, where\n",
    "each cell represents the correlation coefficient between two variables. You can also use other visual tools, like **seaborn**\n",
    "or **matplotlib**, to create heatmaps of the correlation matrix, making it easier to interpret the relationships between\n",
    "variables. For example, the following code demonstrates how to compute and visualize the correlation matrix:\n",
    "'''\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Example data\n",
    "x = [1, 2, 3, 4, 5]\n",
    "y = [2, 4, 6, 8, 10]\n",
    "\n",
    "# Compute Pearson correlation coefficient and p-value\n",
    "corr_coefficient, p_value = pearsonr(x, y)\n",
    "\n",
    "print(f\"Correlation coefficient: {corr_coefficient}\")\n",
    "print(f\"P-value: {p_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bcb0ea-d2fa-422d-88b4-e0b344b9c3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is causation? Explain difference between correlation and causation with an example.\n",
    "'''\n",
    "Causation refers to a relationship where one event or variable directly influences or causes another to occur. In other\n",
    "words, a change in one variable results in a change in another. This is often described by the phrase \"cause and effect.\n",
    "\" Causation implies that there is a direct cause behind the observed effect.\n",
    "Correlation, on the other hand, refers to a statistical association or relationship between two variables, where they tend to \n",
    "vary together, but without one necessarily causing the other. Correlation only indicates that when one variable changes, the \n",
    "other tends to change in a specific way, but it doesn't prove that one causes the other.\n",
    "For example,\n",
    "consider the relationship between ice cream sales and drowning incidents. Data might show that both increase during summer \n",
    "months. There is a positive correlation between ice cream sales and drowning rates, but that doesn’t mean that buying ice \n",
    "cream causes drowning. The real cause is likely the warmer weather, which leads to both increased ice cream consumption and \n",
    "more people swimming, which in turn raises the likelihood of drowning incidents. This is a classic case of a spurious \n",
    "correlation, where two variables are correlated, but there is no causal relationship between them.\n",
    "the key difference is that correlation indicates a relationship between variables, while causation means that one variable\n",
    "directly impacts or causes the other. Correlation does not imply causation.\n",
    "SOME MORE EXAMPLE\n",
    "Example of Correlation\n",
    "Imagine a study that finds a strong correlation between the number of hours spent watching television and the number of fast\n",
    "food meals consumed. The data shows that as people watch more television, they tend to eat more fast food. However, this does\n",
    "not mean that watching television causes people to eat fast food. The correlation exists because both activities are often\n",
    "linked by other factors, like a sedentary lifestyle or the availability of fast food ads during television programming. Thus, \n",
    "the two variables are correlated, but one does not directly cause the other.\n",
    "\n",
    "Example of Causation\n",
    "On the other hand, consider the relationship between smoking and lung cancer. Numerous studies have shown a causal link\n",
    "between smoking and an increased risk of developing lung cancer. In this case, smoking directly causes the development of\n",
    "lung cancer due to the harmful chemicals in tobacco smoke. This is a clear example of causation because changes in one\n",
    "variable (smoking) result in a specific effect (lung cancer), and the relationship is backed by extensive scientific evidence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e128cd-33d2-4619-915d-3cd481ba63fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
    "'''\n",
    "An optimizer in machine learning (ML) or deep learning (DL) is an algorithm used to minimize or maximize an objective function,\n",
    "typically the loss function (also known as the cost function). The goal of the optimizer is to adjust the weights (parameters)\n",
    "of the model during training to reduce the loss and improve the model’s performance. Optimizers work by iteratively updating \n",
    "the model parameters based on the gradients of the loss function with respect to those parameters.\n",
    "Types of Optimizers:\n",
    "There are several types of optimizers, each with its own strengths and weaknesses. Here are some common ones:\n",
    "\n",
    " Gradient Descent (GD)\n",
    "Gradient Descent is the most basic optimizer and works by computing the gradient (derivative) of the loss function with respect \n",
    "to each parameter, and then updating the parameters in the opposite direction of the gradient to minimize the loss.\n",
    "FORMULA = θ=θ−α⋅∇J(θ)\n",
    "where:\n",
    "θ is the model parameter\n",
    "α is the learning rate\n",
    "∇J(θ) is the gradient of the loss function.\n",
    "Example:\n",
    "In training a linear regression model, Gradient Descent would compute the derivative of the loss (e.g., Mean Squared Error)\n",
    "with respect to the model's weights and update them iteratively to reduce the error.\n",
    "Drawback:\n",
    "Slow convergence: If the learning rate is not carefully tuned, GD can either converge too slowly or overshoot the minimum.\n",
    "Stochastic Gradient Descent (SGD)\n",
    "Stochastic Gradient Descent is a variant of Gradient Descent that updates the parameters based on a single training example\n",
    "(or a small random subset of data points, called a mini-batch) at each iteration. This reduces computation time but introduces\n",
    "more variability in the updates.\n",
    "FORMULA = θ=θ−α⋅∇J(θ;xi)\n",
    "where \n",
    "​xi is the individual training example\n",
    "Example:\n",
    "For a deep neural network, instead of using the entire dataset for each update, SGD would randomly sample a data point and \n",
    "compute the gradient for that point, updating the weights accordingly.\n",
    "Drawback:\n",
    "High variance: Because it uses a single example to update parameters, the updates can be noisy and unstable, requiring a \n",
    "lower learning rate.\n",
    "Momentum\n",
    "Momentum is an extension of gradient descent that helps accelerate convergence by considering the past gradients and adding\n",
    "a fraction of the previous update to the current one. This can help smooth out oscillations and lead to faster convergence, \n",
    "especially in regions with shallow gradients.\n",
    "Example:\n",
    "In training a deep neural network, Momentum helps the model to avoid getting stuck in local minima or oscillating in regions \n",
    "with noisy gradients.\n",
    "Advantage:\n",
    "Faster convergence and smoother updates\n",
    "Nesterov Accelerated Gradient (NAG)\n",
    "Nesterov Accelerated Gradient is an enhancement of the momentum method. Instead of updating the parameters based on the \n",
    "current gradient, NAG first makes a \"look-ahead\" step in the direction of the momentum and then computes the gradient at that\n",
    "point. This helps to get a more accurate direction and often leads to faster convergence\n",
    "Example:\n",
    "In training a recurrent neural network (RNN), NAG would use both past gradients and a \"look-ahead\" of the momentum to update\n",
    "weights more effectively.\n",
    "Advantage:\n",
    "More accurate updates and faster convergence compared to momentum.\n",
    "Adagrad (Adaptive Gradient Algorithm)\n",
    "Adagrad is an adaptive optimizer that adjusts the learning rate for each parameter based on the historical gradients. It \n",
    "works by giving more weight to parameters with infrequent updates and less weight to parameters with frequent updates.\n",
    "Example:\n",
    "In a text classification problem using word embeddings, Adagrad would adjust the learning rate for each word in the vocabulary\n",
    "based on how frequently it appears in the training data.\n",
    "Advantage:\n",
    "Adaptive learning rates that help handle sparse data.\n",
    "RMSprop (Root Mean Square Propagation)\n",
    "RMSprop is an improvement of Adagrad that uses a moving average of the squared gradients instead of the sum of all past\n",
    "gradients. This helps avoid the issue of rapidly diminishing learning rates and keeps the learning rate more stable.\n",
    "Example:\n",
    "In training a deep neural network for image classification, RMSprop is effective because it adapts the learning rate based on\n",
    "the gradients and stabilizes updates, especially when training on noisy or highly variable data.\n",
    "Advantage:\n",
    "Works well for non-stationary objectives (e.g., training deep neural networks with noisy gradients)\n",
    "Adam (Adaptive Moment Estimation)\n",
    "Adam combines the ideas of Momentum and RMSprop. It maintains both the moving average of gradients and the moving average of\n",
    "squared gradients. It computes adaptive learning rates for each parameter by using estimates of both first-order momentum and \n",
    "second-order acceleration (RMSprop).\n",
    "Example\n",
    "Adam is widely used in training complex deep learning models, like convolutional neural networks (CNNs) or transformers, as\n",
    "it adapts the learning rate for each parameter individually and handles sparse gradients well.\n",
    "Advantage:\n",
    "Efficient and widely applicable, suitable for a variety of tasks and neural network architectures\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "393bbf2d-482b-45ee-98a5-d0caac04ece4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[139.5475584  179.51720835 134.03875572 291.41702925 123.78965872]\n",
      "Model R-squared: 0.4526027629719195\n"
     ]
    }
   ],
   "source": [
    "#What is sklearn.linear_model ?\n",
    "'''\n",
    "sklearn.linear_model is a module in scikit-learn (a popular machine learning library in Python) that provides various linear\n",
    "models for machine learning tasks, particularly for regression and classification. These models assume that there is a linear\n",
    "relationship between the input variables (features) and the output variable (target). Linear models are widely used due to \n",
    "their simplicity, interpretability, and effectiveness in many applications.\n",
    "Key Models in sklearn.linear_model:\n",
    "Linear Regression (LinearRegression)\n",
    "\n",
    "Use Case: Predicting a continuous target variable based on one or more input features.\n",
    "Example: Predicting house prices based on features like size, location, and age.\n",
    "Description: This model tries to fit the best line (in multiple dimensions if necessary) to the data by minimizing the sum of\n",
    "squared residuals (differences between predicted and actual values.\n",
    "'''\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_diabetes\n",
    "import numpy as np\n",
    "\n",
    "# Load a sample dataset (Diabetes dataset here for regression)\n",
    "data = load_diabetes()\n",
    "X = data.data  # Features\n",
    "y = data.target  # Target (continuous values)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Print the predictions (for example, first 5 predictions)\n",
    "print(predictions[:5])\n",
    "\n",
    "# Optionally, print the model's performance (e.g., R-squared)\n",
    "print(f\"Model R-squared: {model.score(X_test, y_test)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88af60c8-f853-4a58-9d35-1f683548cff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.]\n"
     ]
    }
   ],
   "source": [
    "#What does model.fit() do? What arguments must be given?\n",
    "'''\n",
    "The model.fit() method in scikit-learn is used to train a machine learning model on a given dataset. Specifically, it fits\n",
    "the model to the training data, which means it adjusts the internal parameters (like weights in a linear regression or \n",
    "decision boundaries in a classifier) based on the input data to minimize the error or optimize the objective function.\n",
    "What does model.fit() do?\n",
    "Fitting the Model:\n",
    "When you call fit(), the model learns from the data. For example:\n",
    "In linear regression, the model tries to find the best-fit line that minimizes the residual sum of squares (RSS) between the\n",
    "predicted and actual target values.\n",
    "In classification, the model tries to determine the decision boundary that best separates the classes.\n",
    "Training Process:\n",
    "During training, the model adjusts its internal parameters to make accurate predictions on unseen data (i.e., the test set).\n",
    "This involves computing gradients, optimizing a loss function, and updating the model parameters based on those calculations\n",
    "(depending on the algorithm).\n",
    "Required Arguments for model.fit():\n",
    "The method fit(X, y) requires two primary arguments:\n",
    "\n",
    "X:\n",
    "This is the input features (also called \"predictors\" or \"independent variables\").\n",
    "It is usually a 2D array or matrix with shape (n_samples, n_features), where:\n",
    "n_samples is the number of data points (rows).\n",
    "n_features is the number of features (columns) for each data point.\n",
    "In Python, this is often a NumPy array or a Pandas DataFrame.\n",
    "\n",
    "y:\n",
    "This is the target variable (also called the \"response\" or \"dependent variable\").\n",
    "For regression tasks, y is a 1D array of continuous values (shape: (n_samples,)).\n",
    "For classification tasks, y is a 1D array of labels (class values) corresponding to each sample\n",
    "Example\n",
    "'''\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "# Example data\n",
    "X = np.array([[1], [2], [3], [4], [5]])  # Features (5 samples, 1 feature)\n",
    "y = np.array([1, 2, 3, 4, 5])  # Target values (5 samples)\n",
    "\n",
    "# Create the model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model on the data\n",
    "model.fit(X, y)  # Fit the model to the data\n",
    "\n",
    "# After fitting, you can make predictions\n",
    "predictions = model.predict([[6]])\n",
    "print(predictions)  # Predict for a new value, e.g., 6\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "687f74f0-bda9-4db9-b882-3c20b0900229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [2.]\n",
      "Actual values: [2]\n"
     ]
    }
   ],
   "source": [
    "#What does model.predict() do? What arguments must be given?\n",
    "'''\n",
    "The model.predict() method in scikit-learn is used to make predictions based on the data after a model has been trained using\n",
    "the fit() method. Once you have a trained model (i.e., after calling fit()), you can use predict() to estimate the target\n",
    "values (labels for classification or continuous values for regression) for new, unseen data.\n",
    "\n",
    "What does model.predict() do?\n",
    "Prediction for New Data:\n",
    "The predict() method takes in a set of input features and applies the learned parameters (coefficients, weights, decision\n",
    "boundaries, etc.) to make predictions.\n",
    "For regression models, it predicts continuous numerical values.\n",
    "For classification models, it predicts class labels (discrete categories).\n",
    "Output:\n",
    "The output of predict() is an array of predictions for the given input data.\n",
    "In regression, the output is a continuous value (e.g., predicted price, predicted temperature).\n",
    "In classification, the output is the predicted class label (e.g., \"spam\" or \"not spam\", or the index of the predicted class)\n",
    "Required Argument for model.predict()\n",
    "The predict() method requires one argument:\n",
    "\n",
    "X:\n",
    "This is the input data (features) for which you want to make predictions.\n",
    "Shape: X should be a 2D array (or a DataFrame) with shape (n_samples, n_features), where:\n",
    "n_samples: the number of samples (data points).\n",
    "n_features: the number of features (variables) for each sample\n",
    "If you're making predictions for a single sample, X should still be a 2D array (even if there is only one feature)\n",
    "Example\n",
    "'''\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "# Example data (features and target)\n",
    "X = np.array([[1], [2], [3], [4], [5]])  # Features (5 samples, 1 feature)\n",
    "y = np.array([1, 2, 3, 4, 5])  # Target values (5 samples)\n",
    "\n",
    "# Train-test split (80% for training, 20% for testing)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Print the predictions\n",
    "print(\"Predictions:\", predictions)\n",
    "\n",
    "# Optionally, you can compare with the actual test labels\n",
    "print(\"Actual values:\", y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b78e22-05cd-4d3e-b24a-6d48480e6411",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What are continuous and categorical variables?\n",
    "'''\n",
    "**Continuous variables** are numeric variables that can take an infinite number of values within a given range. They represent\n",
    "quantities or measurements and are typically associated with real numbers. Examples of continuous variables include **height**\n",
    ", **weight**, **temperature**, **age**, and **income**, which can take any value within a specific range and often have decimal\n",
    "precision (e.g., 23.5°C, 70.2 kg). These variables allow meaningful arithmetic operations like addition, subtraction, and\n",
    "multiplication, and are generally handled as numeric data in machine learning models, where scaling or normalization may be \n",
    "necessary.\n",
    "On the other hand, **categorical variables** represent distinct categories or groups and are non-numeric. These variables take\n",
    "a limited number of values, and the values are usually text or labels, such as **gender** (Male, Female), **marital status**\n",
    "(Single, Married), or **product type** (Electronics, Clothing). Categorical variables can be further classified as **nominal**\n",
    ", where categories have no natural order (e.g., color or country), and **ordinal**, where categories have a meaningful order\n",
    "(e.g., education level or rating scale). Categorical variables must be encoded into numeric format (e.g., using label encoding\n",
    "or one-hot encoding) before they can be used in machine learning models, which typically require numerical input for\n",
    "computation.\n",
    "The key difference between these types of variables is that continuous variables represent measurable quantities that can take\n",
    "any value within a range, while categorical variables represent discrete groups or categories. Continuous variables are often\n",
    "used directly in regression tasks, while categorical variables are commonly used in classification tasks after proper\n",
    "encoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6fbaad2b-901f-4170-a914-cd54e91d9dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.16017749 -1.28717133]\n",
      " [-0.98829935 -0.83815808]\n",
      " [ 0.         -0.08980265]\n",
      " [ 0.64454305  0.80822386]\n",
      " [ 1.50393379  1.4069082 ]]\n"
     ]
    }
   ],
   "source": [
    "#What is feature scaling? How does it help in Machine Learning?\n",
    "'''\n",
    "\n",
    "Feature scaling is the process of standardizing or normalizing the range of independent variables (features) in a dataset.\n",
    "It is a crucial step in preprocessing the data for machine learning models, especially when the features have different units\n",
    "or scales. The goal of feature scaling is to transform the data such that each feature contributes equally to the model’s\n",
    "learning process.\n",
    "Why is Feature Scaling Important in Machine Learning?\n",
    "\n",
    "Equal Contribution of Features: Machine learning algorithms that rely on distance calculations, such as k-Nearest Neighbors\n",
    "(k-NN), Support Vector Machines (SVM), and K-means clustering, are sensitive to the scale of the features. Features with\n",
    "larger numerical values (like income in thousands or distances in kilometers) can dominate the learning process, overshadowing\n",
    "features with smaller numerical values (like age or ratings). Feature scaling ensures that all features contribute equally\n",
    "to the model's learning.\n",
    "\n",
    "Improved Convergence in Optimization Algorithms: Many machine learning algorithms, particularly those based on gradient \n",
    "descent (e.g., Linear Regression, Logistic Regression, and Neural Networks), benefit from feature scaling. If features\n",
    "have very different scales, the optimization process can take longer to converge, or it might get stuck in suboptimal \n",
    "solutions. Scaling helps gradient-based optimization algorithms converge faster and more efficiently.\n",
    "\n",
    "Improved Performance of Regularization: In regularized models like Ridge Regression or Lasso Regression, feature scaling is\n",
    "important because the regularization term penalizes large coefficients. If the features are on different scales, the \n",
    "regularization penalizes features equally.\n",
    "\n",
    "Assumptions in Certain Algorithms: Some machine learning algorithms, like Principal Component Analysis (PCA) and Linear\n",
    "Discriminant Analysis (LDA), make assumptions about the data. For example, PCA assumes that features are centered and have \n",
    "comparable scales. If the features are on different scales, PCA may give more importance to features with larger variances,\n",
    "which can skew the analysise regularization penalizes features equally.\n",
    "\n",
    "The two main types of feature scaling in machine learning are normalization and standardization,\n",
    "Normalization    \n",
    "Scales each feature to a range of 0 to 1\n",
    "Ensures that all features contribute equally to the analysis \n",
    "A fundamental technique in machine learning\n",
    "The formula for normalization is: \\((x-min(X))/(max(X)-min(X))\\)       \n",
    "Standardization\n",
    "Scales each feature to have a mean of 0 and a standard deviation of 1\n",
    "Useful when data follows a normal distribution\n",
    "Emphasizes relative distances from the mean \n",
    "The formula for standardization is: \\(Z=(x-\\mu )/\\sigma \\) \n",
    "Why use feature scaling?\n",
    "Feature scaling improves model performance\n",
    "It can speed up the convergence of iterative optimization algorithms\n",
    "It makes the optimization process more stable and efficient\n",
    "It helps neural networks and deep learning models perform better\n",
    "Example\n",
    "'''\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Example data: Age (18-80), Income (10000-100000)\n",
    "X = np.array([[18, 10000], [22, 25000], [45, 50000], [60, 80000], [80, 100000]])\n",
    "\n",
    "# Standardization (Z-score scaling)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(X_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1fe1918-d66c-46ce-9f04-6b3358b7ddd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized Data:\n",
      "[[-1.16017749 -1.28717133]\n",
      " [-0.98829935 -0.83815808]\n",
      " [ 0.         -0.08980265]\n",
      " [ 0.64454305  0.80822386]\n",
      " [ 1.50393379  1.4069082 ]]\n"
     ]
    }
   ],
   "source": [
    "#How do we perform scaling in Python?\n",
    "'''\n",
    "In Python, scaling of features is typically done using the scikit-learn library, which provides two main functions for\n",
    "scaling: StandardScaler for standardization (Z-score scaling) and MinMaxScaler for normalization (Min-Max scaling).\n",
    "Both of these are commonly used to preprocess the data before applying machine learning algorithms.\n",
    " Standardization (Z-score scaling) with StandardScaler\n",
    " Example\n",
    " '''\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Example data: Age (18-80), Income (10000-100000)\n",
    "X = np.array([[18, 10000], [22, 25000], [45, 50000], [60, 80000], [80, 100000]])\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the data and transform it\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Print the scaled data\n",
    "print(\"Standardized Data:\")\n",
    "print(X_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ffb36f62-96d3-48a2-a718-189c85309fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Data:\n",
      "[[0.         0.        ]\n",
      " [0.06451613 0.16666667]\n",
      " [0.43548387 0.44444444]\n",
      " [0.67741935 0.77777778]\n",
      " [1.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "#. Normalization (Min-Max scaling) with MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Example data: Age (18-80), Income (10000-100000)\n",
    "X = np.array([[18, 10000], [22, 25000], [45, 50000], [60, 80000], [80, 100000]])\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler to the data and transform it\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Print the scaled data\n",
    "print(\"Normalized Data:\")\n",
    "print(X_scaled)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b8507d40-5980-4219-9f13-09ef133b7ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled Training Data:\n",
      "[[ 1.29307426  1.17953565]\n",
      " [-0.25419409 -0.29488391]\n",
      " [-1.44780109 -1.47441956]\n",
      " [ 0.40892092  0.58976782]]\n",
      "Scaled Test Data:\n",
      "[[-1.27097043 -1.03209369]]\n"
     ]
    }
   ],
   "source": [
    "#Scaling on Training and Test Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Example data\n",
    "X = np.array([[18, 10000], [22, 25000], [45, 50000], [60, 80000], [80, 100000]])\n",
    "y = np.array([0, 1, 0, 1, 0])  # Example target labels\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler only on the training data and transform it\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using the same scaler (note: don't call fit on test data)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Scaled Training Data:\")\n",
    "print(X_train_scaled)\n",
    "\n",
    "print(\"Scaled Test Data:\")\n",
    "print(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0a156e78-a731-443e-b5c6-64f4d21ad76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other Scaling Techniques\n",
    "'''\n",
    "RobustScaler: This method scales features using the median and interquartile range, making it robust to outliers. It’s useful\n",
    "when the data has many outliers that you don't want to affect the scaling.\n",
    "Example\n",
    "'''\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c31fd5d1-64ec-4330-a6ee-cb4828cac84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MaxAbsScaler: Scales each feature by its maximum absolute value, ensuring that the transformed values lie between -1 and 1.\n",
    "'''This is useful for sparse data\n",
    "Example\n",
    "'''\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "scaler = MaxAbsScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "893e6e74-44b9-4ece-b6d8-271be89fafae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Color  Color_encoded\n",
      "0    Red              2\n",
      "1  Green              2\n",
      "2   Blue              1\n",
      "3  Green              2\n",
      "4    Red              2\n"
     ]
    }
   ],
   "source": [
    "#Explain data encoding?\n",
    "'''\n",
    "Data encoding is the process of converting data into a coded form using a set of rules or algorithms to ensure efficient\n",
    "and accurate transmission, storage, or processing.\n",
    "Types of Data Encoding\n",
    "1. *Text Encoding*: Converting text data into a numerical representation using character sets like ASCII, Unicode, or UTF-8.\n",
    "2. *Image Encoding*: Compressing image data using algorithms like JPEG, PNG, or GIF.\n",
    "3. *Audio Encoding*: Compressing audio data using algorithms like MP3, AAC, or WAV.\n",
    "4. *Video Encoding*: Compressing video data using algorithms like H.264, MPEG-4, or VP9\n",
    "Encoding Techniques\n",
    "1. *Lossless Encoding*: No data is lost during compression (e.g., ZIP, gzip).\n",
    "2. *Lossy Encoding*: Some data is lost during compression (e.g., JPEG, MP3).\n",
    "3. *Run-Length Encoding (RLE)*: Replacing sequences of identical bytes with a single byte and a count.\n",
    "4. *Huffman Coding*: Assigning shorter codes to more frequently occurring bytes.\n",
    "Purpose of Data Encoding\n",
    "1. *Compression*: Reducing data size for efficient storage or transmission.\n",
    "2. *Error Detection and Correction*: Adding redundancy to detect and correct errors.\n",
    "3. *Security*: Protecting data from unauthorized access or tampering.\n",
    "4. *Format Conversion*: Converting data between different formats or platforms.\n",
    "Common Encoding Standards\n",
    "1. *ASCII (American Standard Code for Information Interchange)*\n",
    "2. *UTF-8 (Unicode Transformation Format - 8-bit)*\n",
    "3. *Base64*: Encoding binary data as text using a 64-character alphabet\n",
    "4. *MIME (Multipurpose Internet Mail Extensions)*: Encoding email attachments and messages\n",
    "Example\n",
    "'''\n",
    "import pandas as pd\n",
    "\n",
    "# Example data\n",
    "data = pd.DataFrame({\"Color\": [\"Red\", \"Green\", \"Blue\", \"Green\", \"Red\"]})\n",
    "\n",
    "# Perform Frequency Encoding\n",
    "frequency_encoding = data[\"Color\"].value_counts().to_dict()\n",
    "data[\"Color_encoded\"] = data[\"Color\"].map(frequency_encoding)\n",
    "\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832eac55-f559-43fa-8d18-834c6c36c648",
   "metadata": {},
   "source": [
    "THANK YOU"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-ai-2024.04-py310",
   "language": "python",
   "name": "conda-env-anaconda-ai-2024.04-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
